import numpy as np
from itertools import combinations

def most_similar(embeddings):
    """
    Compute cosine similarity between all pairs of embeddings
    and return the pair of phrases with the highest similarity.
    """
    # Convert lists to numpy arrays for vector operations
    vectors = {k: np.array(v) for k, v in embeddings.items()}

    max_sim = -1.0
    most_similar_pair = None

    # Iterate over all unique pairs
    for (phrase1, vec1), (phrase2, vec2) in combinations(vectors.items(), 2):
        # Compute cosine similarity
        sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        
        # Update if this pair is more similar
        if sim > max_sim:
            max_sim = sim
            most_similar_pair = (phrase1, phrase2)

    return most_similar_pair, max_sim


# Example usage:
if __name__ == "__main__":
    embeddings = {"Packaging was excellent.":[-0.01674579456448555,-0.06481242924928665,-0.24050545692443848,0.042519159615039825,0.14857585728168488,-0.11343036592006683,0.1299005150794983,0.17366009950637817,-0.12356054037809372,0.049548257142305374,0.23058201372623444,-0.015152188017964363,-0.06047092750668526,-0.08428027480840683,0.140513077378273,0.0330953411757946,0.15987755358219147,-0.13982394337654114,-0.1899235099554062,0.0849694088101387,0.10901995003223419,0.023171907290816307,0.1423737108707428,-0.010603947564959526,-0.12362945079803467,-0.02598010189831257,0.04410415142774582,-0.0650191679596901,0.13754981756210327,0.06319297850131989,0.2340276539325714,-0.1448545753955841,0.5634305477142334,0.003012778703123331,-0.15422670543193817,-0.10137064009904861,0.10013020783662796,0.05392421782016754,0.10895103961229324,-0.017710573971271515,-0.0018617206951603293,0.01796899549663067,0.0550268217921257,0.251669317483902,-0.005680993665009737,0.12080402672290802,-0.08173050731420517,0.1045406237244606,0.040589600801467896,0.1787596344947815],"The product did not meet my expectations.":[-0.0789492279291153,-0.017544273287057877,-0.20415154099464417,0.05229542776942253,-0.33714449405670166,-0.0982111245393753,0.12587708234786987,0.11225880682468414,-0.0027278736233711243,0.023417923599481583,-0.13826850056648254,-0.291504830121994,-0.18145440518856049,-0.02094884216785431,0.16108831763267517,0.11158403009176254,-0.012337733991444111,-0.12710395455360413,-0.3081902861595154,0.03130057826638222,0.03367764130234718,0.21053127944469452,-0.07563666999340057,-0.1394953727722168,-0.22488567233085632,0.02143959142267704,0.15299096703529358,-0.07631145417690277,0.011356235481798649,0.15188677608966827,0.042173732072114944,-0.1614563763141632,0.12152169644832611,-0.29862070083618164,0.09680021554231644,0.09864052385091782,0.11354702711105347,-0.026837829500436783,0.0004603167180903256,0.1484515368938446,0.014362072572112083,0.04327791556715965,-0.09661618620157242,0.026745814830064774,-0.12047885358333588,0.252981036901474,0.135446697473526,-0.1340971291065216,0.08907092362642288,-0.11428314447402954],"The quality exceeds the price.":[-0.050457071512937546,-0.034066375344991684,-0.10696785151958466,-0.03518003225326538,0.11867549270391464,-0.08566565811634064,-0.017789902165532112,0.3559122681617737,-0.04817265644669533,-0.14437519013881683,0.14620272815227509,0.015005768276751041,-0.34517550468444824,0.022687122225761414,0.2908063530921936,0.17681391537189484,-0.20993797481060028,-0.286237508058548,-0.022829897701740265,0.04428914561867714,0.08435212075710297,0.04840109869837761,-0.03081108257174492,-0.04203328490257263,-0.0324387289583683,-0.23552344739437103,0.0033713006414473057,0.02891216054558754,0.0676758736371994,-0.11616263538599014,0.05408358573913574,-0.18183964490890503,0.23757942020893097,-0.34266263246536255,-0.19920121133327484,-0.021787632256746292,0.0973161906003952,0.032724279910326004,0.07030294835567474,-0.1132500022649765,0.09360401332378387,0.028341054916381836,-0.09657375514507294,0.1291838139295578,0.12198790162801743,0.019260495901107788,0.02211601845920086,0.058595310896635056,-0.07481467723846436,0.012935514561831951],"Shipping costs were too high.":[-0.02132924273610115,-0.05078135058283806,0.24659079313278198,0.03407837450504303,-0.031469374895095825,0.04534817487001419,-0.14255358278751373,0.028483819216489792,-0.0895128846168518,0.05390138924121857,-0.0863390564918518,0.025431020185351372,-0.10597378760576248,0.02617068588733673,0.04362677410244942,-0.020603027194738388,0.1553564965724945,-0.12254228442907333,-0.3750503957271576,0.08009897172451019,0.13728179037570953,0.17526021599769592,-0.08456385880708694,-0.21130205690860748,-0.06810295581817627,0.008573387749493122,0.2928534746170044,-0.27736085653305054,0.12576991319656372,-0.23002229630947113,0.1522364616394043,-0.13523761928081512,0.16622285544872284,-0.1358831524848938,-0.32512974739074707,0.04222813621163368,-0.11146076023578644,0.23475615680217743,0.1606282889842987,0.07009332627058029,-0.08875977247953415,-0.0171198770403862,0.1295354813337326,0.033890094608068466,0.039941899478435516,0.14147770404815674,0.10349927842617035,-0.037790145725011826,0.022405119612812996,-0.013334139250218868],"I love the variety of products available.":[0.1263255476951599,-0.3116876780986786,-0.1845686137676239,0.14346520602703094,0.025372233241796494,-0.2828041911125183,0.09950517863035202,0.23424185812473297,-0.03733427822589874,0.0246580820530653,0.15838304162025452,-0.19409063458442688,-0.16615936160087585,0.07708873599767685,0.03473556041717529,-0.08458733558654785,-0.18012499809265137,0.1893296241760254,-0.09109405428171158,0.08065950125455856,0.08831679821014404,0.04641987755894661,-0.13743458688259125,-0.18075980246067047,-0.01637590117752552,-0.14092598855495453,-0.23630495369434357,0.06447205692529678,0.07486693561077118,-0.08181007951498032,0.06530523300170898,-0.21678480505943298,-0.06542425602674484,0.021603098139166832,0.005911591462790966,0.1277538537979126,-0.004547759424895048,0.05074446648359299,0.32470110058784485,-0.08546018600463867,-0.04284911975264549,0.07546205818653107,0.202660471200943,-0.08553953468799591,0.00024378496163990349,-0.03582662343978882,-0.29058051109313965,-0.08950705081224442,0.03743346780538559,-0.06633678823709488]}
    result = most_similar(embeddings)
    print("Most similar pair:", result[0])
    print("Cosine similarity:", round(result[1], 4))
